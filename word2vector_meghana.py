# -*- coding: utf-8 -*-
"""word2vector_Meghana

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IUQqNPEnUEuqrQJhH8eoLAc-lm144erd

NOTE: The word "teachers" is not present in the provided context,
so we compute similarities ONLY for ("students","learners") and ("projects","assignments").
"""

!pip install gensim scikit-learn

import re
import numpy as np
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity

np.set_printoptions(precision=3, suppress=True)

context = """
Innomatics Research Labs is one of the leading EdTech companies in India that focuses on skill-based learning.
It offers training programs in Data Science, Artificial Intelligence, Machine Learning, and Full Stack Web Development.
The curriculum is designed by industry experts and focuses on hands-on practice through projects and assignments.

Students learn Python, SQL, Power BI, and other essential tools required for analytics and development roles.
Every week, mentors conduct live doubt sessions to help learners clarify technical concepts.
Innomatics provides a Learning Management System (LMS) where students can access recorded lectures, notes, and quizzes.

Career guidance is a major part of the program.
The placement team conducts resume workshops, mock interviews, and connects students with hiring partners from top companies.
Many learners have successfully transitioned into data analyst, machine learning engineer, and full-stack developer roles after completing the course.

The mentors at Innomatics are experienced professionals from the IT industry.
They encourage students to participate in hackathons, coding challenges, and capstone projects.
Students also get personalized mentoring sessions to strengthen their portfolios and LinkedIn profiles.

Innomatics frequently collaborates with organizations to host guest lectures and webinars on trending topics like Generative AI, LangChain, and Prompt Engineering.
The training focuses on real-world case studies, making learners ready for corporate challenges.

The organization believes in the motto “Learn, Innovate, and Grow”.
Innomatics also promotes a supportive learning culture that helps students stay motivated and confident during their learning journey.
The LMS allows learners to track progress, complete assignments, and interact with mentors and peers.

Students appreciate the structured course flow, dedicated support, and practical exposure they get during training.
Overall, Innomatics Research Labs continues to bridge the gap between education and employment through its innovative teaching methods and community-driven approach.
"""
print("Loaded context characters:", len(context))

def normalize_and_tokenize(text: str):
  #only considering lowercase + keep letters/spaces
    text = text.lower()
    text = re.sub(r'[^a-z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text.split()

#splitting sentences here
sentences_raw = [s.strip() for s in re.split(r'[.!?]\s+', context) if s.strip()]
tokenized_sentences = [normalize_and_tokenize(s) for s in sentences_raw if s]

print("Num sentences:", len(tokenized_sentences))
print("First sentence (tokens):", tokenized_sentences[0][:15], "...")
assert len(tokenized_sentences) > 0, "tokenization failed"

VECTOR_SIZE = 100
WINDOW = 5
MIN_COUNT = 1
EPOCHS = 120
SEED = 42  #keeping it reproducible so our numbers don't jump around

model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=VECTOR_SIZE,
    window=WINDOW,
    min_count=MIN_COUNT,
    workers=2,
    sg=0,          #CBOW
    epochs=EPOCHS,
    seed=SEED
)

print("Vocab size:", len(model.wv.key_to_index))
print("Sample vocab:", list(model.wv.key_to_index.keys())[:15])

pairs = [
    ("students", "learners"),
    ("projects", "assignments"),
    ('mentors','teachers')
]

print("\n Word Similarity ")
for a, b in pairs:
    ok = (a in model.wv.key_to_index) and (b in model.wv.key_to_index)
    print(f"Pair: {a!r} vs {b!r} | in-vocab? {ok}")
    if ok:
        sim = float(model.wv.similarity(a, b))
        print(f" similarity = {sim:.4f}")
    else:
        print("skipping this since one of them not in vocab")

docs = [
    "Students use the LMS to access lectures, notes, and quizzes.",
    "Mentors conduct live doubt sessions and guide learners.",
    "Projects and assignments provide practical exposure for real-world skills.",
    "The placement team conducts mock interviews and resume workshops.",
    "Innomatics hosts webinars on Generative AI and Prompt Engineering."
]

tokenized_docs = [normalize_and_tokenize(d) for d in docs]
print("Tokenized doc 1 preview:", tokenized_docs[0])

def doc_to_vec(tokens, wv):
    #averaging word vectors
    vecs = [wv[w] for w in tokens if w in wv.key_to_index]
    if len(vecs) == 0:
        return np.zeros(wv.vector_size, dtype=np.float32)
    return np.mean(np.vstack(vecs), axis=0)

doc_vectors = np.vstack([doc_to_vec(toks, model.wv) for toks in tokenized_docs])
print("doc_vectors shape =", doc_vectors.shape)

model.save("innomatics_word2vec_cbow.model")
np.save("innomatics_doc_vectors.npy", doc_vectors)
print("Saved model + doc vectors")

